{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/H605-oss/Basic_Calculator/blob/main/Notebooks/Cricket_Shot_Classification_EfNetB0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4xKBUk5fcPf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9181ffc-cd82-42b6-db25-f033262e88d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deap in /usr/local/lib/python3.11/dist-packages (1.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deap) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install deap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nDhs-aqO4c7E"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Dense,Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "from deap import base, creator, tools, algorithms\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "import random\n",
        "import pathlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0Wm8FI5WcPgB"
      },
      "outputs": [],
      "source": [
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.uint8)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame.numpy()\n",
        "\n",
        "\n",
        "def frames_from_video_file(video_path, n_frames, output_size=(224, 224), frame_step=1):\n",
        "    \"\"\"\n",
        "    Extracts frames sequentially from the start of the video file, with a specified step between frames.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image (height, width).\n",
        "      frame_step: Number of frames to skip between extracted frames.\n",
        "\n",
        "    Returns:\n",
        "      A NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    src = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "    src.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Start from the first frame\n",
        "\n",
        "    # Attempt to read the first frame\n",
        "    ret, frame = src.read()\n",
        "    if ret:\n",
        "        frame = format_frames(frame, output_size)\n",
        "        result.append(frame)\n",
        "    else:\n",
        "        # If the first frame can't be read, append a zero frame and exit\n",
        "        result.append(np.zeros((output_size[0], output_size[1], 3), dtype=np.uint8))\n",
        "\n",
        "    # Read subsequent frames with the specified frame_step\n",
        "    for _ in range(n_frames - 1):\n",
        "        for _ in range(frame_step):\n",
        "            ret, frame = src.read()\n",
        "        if ret:\n",
        "            frame = format_frames(frame, output_size)\n",
        "            result.append(frame)\n",
        "        else:\n",
        "            # Append a zero-like frame if no more frames can be read\n",
        "            result.append(np.zeros_like(result[0]))\n",
        "\n",
        "    src.release()\n",
        "\n",
        "    # Convert the list of frames to a NumPy array and adjust color channels from BGR to RGB\n",
        "    result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "    return result\n",
        "\n",
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    \"\"\" Returns a set of frames with their associated label.\n",
        "\n",
        "      Args:\n",
        "        path: Video file paths.\n",
        "        n_frames: Number of frames.\n",
        "        training: Boolean to determine if training dataset is being created.\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.mp4'))\n",
        "    classes = [p.parent.name for p in video_paths]\n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames)\n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZIOXS-mfR_L",
        "outputId": "51085dbc-0597-4efe-a516-3b45b017d7ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0m6srqXEcPgI"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "num_frames = 30\n",
        "\n",
        "# Define paths using pathlib\n",
        "train_path = pathlib.Path('/content/drive/MyDrive/dataset/train/')\n",
        "test_path = pathlib.Path('/content/drive/MyDrive/dataset/test/')\n",
        "val_path = pathlib.Path('/content/drive/MyDrive/dataset/val/')\n",
        "\n",
        "# Update output_signature to match the expected shapes:\n",
        "# - The first element of the tuple describes the input tensor to the model:\n",
        "#   This should be (num_frames, 224, 224, 3) for each video in the batch.\n",
        "# - The second element describes the labels, which should be a scalar value per video (batch).\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(num_frames, 224, 224, 3), dtype=tf.uint8),  # Updated input shape per video\n",
        "    tf.TensorSpec(shape=(), dtype=tf.uint8)  # Scalar integer for the label\n",
        ")\n",
        "\n",
        "# Generator for training data\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: FrameGenerator(train_path, num_frames, training=True)(),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "# Generator for test data\n",
        "test_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: FrameGenerator(test_path, num_frames)(),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "test_ds = test_ds.batch(batch_size)\n",
        "\n",
        "# Generator for validation data\n",
        "val_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: FrameGenerator(val_path, num_frames)(),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "val_ds = val_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "std0kbQucPgI"
      },
      "outputs": [],
      "source": [
        "for frames, labels in train_ds.take(2):\n",
        "  print(labels)\n",
        "  print(f\"Shape: {frames.shape}\")\n",
        "  print(f\"Label: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1A2ltK3kcPgV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jfdN0Fi49YVh"
      },
      "outputs": [],
      "source": [
        "filepath = \"models/GA.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HXCnBnfEcPgX"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EwmYz8FrcPgp"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache()\n",
        "val_ds = val_ds.cache()\n",
        "test_ds = test_ds.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GdHhiUkb9pGR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "f716a5aa-33b8-4a67-caf9-069d26cfee5a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=models/GA.h5",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-917205384.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n\u001b[1;32m      4\u001b[0m                                    \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    185\u001b[0m                     \u001b[0;34m\"When using `save_weights_only=True` in `ModelCheckpoint`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0;34m\", the filepath provided must end in `.weights.h5` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=models/GA.h5"
          ]
        }
      ],
      "source": [
        "model_save = tf.keras.callbacks.ModelCheckpoint(filepath, monitor=\"val_accuracy\", verbose=0, save_best_only=True, save_weights_only=True, mode=\"max\", save_freq=\"epoch\")\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.2,\n",
        "                                   patience=4,\n",
        "                                   verbose=1,\n",
        "                                   min_delta=1e-3,min_lr = 5*1e-12,\n",
        "                                   )\n",
        "\n",
        "callbacks = [model_save,  reduce_lr]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "def build_model(learning_rate):\n",
        "    \"\"\"\n",
        "    Builds a TensorFlow/Keras model with a specified learning rate.\n",
        "\n",
        "    Args:\n",
        "    learning_rate: The learning rate to use for the Adam optimizer.\n",
        "\n",
        "    Returns:\n",
        "    A compiled Keras model.\n",
        "    \"\"\"\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        layers.TimeDistributed(base_model, input_shape=(None, 224, 224, 3)),\n",
        "        layers.TimeDistributed(layers.GlobalAveragePooling2D()),\n",
        "        layers.GRU(256, return_sequences=True),\n",
        "        layers.GRU(128),\n",
        "        layers.Dense(1024, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "nnaL50Wl6gos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from deap import base, creator, tools, algorithms\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "def build_model(learning_rate):\n",
        "    \"\"\"\n",
        "    Builds a TensorFlow/Keras model with a specified learning rate.\n",
        "\n",
        "    Args:\n",
        "    learning_rate: The learning rate to use for the Adam optimizer.\n",
        "\n",
        "    Returns:\n",
        "    A compiled Keras model.\n",
        "    \"\"\"\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        layers.TimeDistributed(base_model, input_shape=(None, 224, 224, 3)),\n",
        "        layers.TimeDistributed(layers.GlobalAveragePooling2D()),\n",
        "        layers.GRU(256, return_sequences=True),\n",
        "        layers.GRU(128),\n",
        "        layers.Dense(1024, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def log_individual(log_df, gen, ind, fit, stage, p1=None, p2=None, mut=None):\n",
        "    new_row = pd.DataFrame([{\n",
        "        \"Generation\": gen,\n",
        "        \"Individual\": str(ind),\n",
        "        \"Learning Rate\": ind[0],\n",
        "        \"Epochs\": ind[1],\n",
        "        \"Fitness\": fit[0] if fit else None,\n",
        "        \"Stage\": stage,\n",
        "        \"Parent 1\": str(p1) if p1 else \"\",\n",
        "        \"Parent 2\": str(p2) if p2 else \"\",\n",
        "        \"Mutation\": str(mut) if mut else \"\"\n",
        "    }])\n",
        "    return pd.concat([log_df, new_row], ignore_index=True)\n",
        "\n",
        "# Initialize your DataFrame before the loop\n",
        "log_df = pd.DataFrame(columns=['Generation', 'Individual', 'Learning Rate', 'Epochs', 'Fitness', 'Stage', 'Parent 1', 'Parent 2', 'Mutation'])\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate_model(individual, callbacks):\n",
        "    learning_rate, num_epochs = float(individual[0]), int(individual[1])\n",
        "    print(f\"Evaluating with learning rate: {learning_rate} and epochs: {num_epochs}\")\n",
        "    model = build_model(learning_rate)\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=num_epochs, callbacks=callbacks, verbose=1)\n",
        "    max_val_acc = max(history.history['val_accuracy'])\n",
        "    print(f\"Validation accuracy: {max_val_acc}\")\n",
        "    return (max_val_acc,)\n",
        "\n",
        "# Initialize DEAP for GA\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_float\", random.uniform, 0.0001, 0.02)\n",
        "toolbox.register(\"attr_int\", random.randint, 1, 20)\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.attr_float, toolbox.attr_int), n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", evaluate_model, callbacks=callbacks)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "# Enhanced logging setup\n",
        "log_df = pd.DataFrame(columns=['Generation', 'Individual', 'Learning Rate', 'Epochs', 'Fitness', 'Stage', 'Parent 1', 'Parent 2', 'Mutation'])\n",
        "\n",
        "# Genetic Algorithm Loop with enhanced logging\n",
        "stagnation_limit = 10  # Allowable number of generations without improvement\n",
        "best_fitness = float('-inf')\n",
        "generations_without_improvement = 0\n",
        "population = toolbox.population(n=10)\n",
        "ngen = 50  # Maximum number of generations\n",
        "\n",
        "for gen in range(ngen):\n",
        "    print(f\"Generation {gen}\")\n",
        "    # Evaluate all individuals with invalid fitness\n",
        "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
        "    fitnesses = list(map(toolbox.evaluate, invalid_ind))\n",
        "    for ind, fit in zip(invalid_ind, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "        log_df = log_individual(log_df, gen, ind, fit, \"Evaluation\")\n",
        "\n",
        "\n",
        "    # Select the next generation individuals\n",
        "    offspring = toolbox.select(population, len(population))\n",
        "    # Clone the selected individuals\n",
        "    offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "    # Apply crossover and mutation on the offspring\n",
        "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "        if random.random() < 0.5:\n",
        "            toolbox.mate(child1, child2)\n",
        "            del child1.fitness.values\n",
        "            del child2.fitness.values\n",
        "            log_df = log_individual(log_df, gen, child1, None, \"Crossover\", p1=child1, p2=child2)\n",
        "            log_df = log_individual(log_df, gen, child2, None, \"Crossover\", p1=child1, p2=child2)\n",
        "\n",
        "\n",
        "    for mutant in offspring:\n",
        "        if random.random() < 0.2:\n",
        "            toolbox.mutate(mutant)\n",
        "            del mutant.fitness.values\n",
        "            log_df = log_individual(log_df, gen, mutant, None, \"Mutation\", mut=mutant)\n",
        "\n",
        "\n",
        "    # The population is entirely replaced by the offspring\n",
        "    population[:] = offspring\n",
        "\n",
        "    # Gather all the fitnesses in one list and print the stats\n",
        "    fits = [ind.fitness.values[0] for ind in population if ind.fitness.valid]\n",
        "\n",
        "    if len(fits) > 0:\n",
        "        length = len(population)\n",
        "        mean = sum(fits) / length\n",
        "        sum2 = sum(x*x for x in fits)\n",
        "        std = abs(sum2 / length - mean**2)**0.5\n",
        "\n",
        "        print(f\"  Min {min(fits)}\")\n",
        "        print(f\"  Max {max(fits)}\")\n",
        "        print(f\"  Avg {mean}\")\n",
        "        print(f\"  Std {std}\")\n",
        "\n",
        "        # Checking for improvement\n",
        "        current_best = max(fits)\n",
        "        if current_best > best_fitness:\n",
        "            best_fitness = current_best\n",
        "            generations_without_improvement = 0\n",
        "        else:\n",
        "            generations_without_improvement += 1\n",
        "    else:\n",
        "        generations_without_improvement += 1\n",
        "\n",
        "\n",
        "    if generations_without_improvement >= stagnation_limit:\n",
        "        print(f\"Stopping early due to no improvement after {stagnation_limit} generations.\")\n",
        "        break\n",
        "\n",
        "# Final logging and output\n",
        "best_ind = tools.selBest(population, 1)[0]\n",
        "print(f\"Best individual is: {best_ind}, with accuracy: {best_ind.fitness.values[0]}\")\n",
        "log_df.to_csv('GA_run_log.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "EmuvHILByJiG",
        "outputId": "4a144130-ddf9-4ee7-d991-2d0da9129b3f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.11/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0\n",
            "Evaluating with learning rate: 0.001172762097285506 and epochs: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'callbacks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-2121895422.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Evaluate all individuals with invalid fitness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-28-2121895422.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(individual)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluating with learning rate: {learning_rate} and epochs: {num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mmax_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation accuracy: {max_val_acc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'callbacks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_df.to_csv('ga_process_log.csv')\n",
        "\n",
        "# Optionally print the DataFrame to check the outputs immediately\n",
        "print(log_df)"
      ],
      "metadata": {
        "id": "WCKSYL6mOkal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeDajZmu9seZ"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = adam, loss = \"sparse_categorical_crossentropy\", metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwYYRiJB9uu4"
      },
      "outputs": [],
      "source": [
        "history =  model.fit(train_ds, validation_data=val_ds, epochs = 20, callbacks = callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufkoqv6w94oA"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Cricket Shot Classification Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('custom_1_acc.png', bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16kbt0v397gR"
      },
      "outputs": [],
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Cricket Shot Classification Model Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('custom_1_loss.png', bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU_RLc9299CI"
      },
      "outputs": [],
      "source": [
        "#Prediction Function\n",
        "array = model.predict(test_ds, batch_size=1, verbose=1)\n",
        "y_pred = np.argmax(array, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj1s2MxT-FzQ"
      },
      "outputs": [],
      "source": [
        "y_true = y = np.concatenate([y for x, y in test_ds], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqwM9e63-Lfg"
      },
      "outputs": [],
      "source": [
        "conf_mat = confusion_matrix(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obp14rJMqi0Y"
      },
      "outputs": [],
      "source": [
        "conf_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xwabWNS-NFQ"
      },
      "outputs": [],
      "source": [
        "classes = {'cover': 0, 'defense': 1, 'flick': 2, 'hook': 3, 'late_cut': 4, 'lofted': 5, 'pull': 6, 'square_cut': 7, 'straight': 8, 'sweep': 9}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdeYrpW6-S93"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "report = metrics.classification_report(y_true, y_pred, target_names=classes)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdirPsAFptxC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    #plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig('confmats/Efnetb0_10/{}1.jpg'.format(title))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX2xTzlmcPhX"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(conf_mat,target_names=classes,title='Circket Shot Confusion Matrix',normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gAvtaSrqXNH"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(conf_mat,target_names=classes,title='Circket Shot Confusion Matrix',normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DheE5X_ocPhY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def classify_video(video_path, model, frame_count, class_labels):\n",
        "    # Process the video file to get the frames\n",
        "    frames = frames_from_video_file(video_path, frame_count)\n",
        "\n",
        "    # Add batch dimension if the model expects it\n",
        "    frames = np.expand_dims(frames, axis=0)\n",
        "\n",
        "    # Use the model to predict the class probabilities\n",
        "    predictions = model.predict(frames)\n",
        "    print(\"Raw predictions:\", predictions)\n",
        "\n",
        "    # Convert predictions to class labels\n",
        "    predicted_class_idx = np.argmax(predictions, axis=1)[0]  # Get the index of the max class score\n",
        "    print(\"Predicted class index:\", predicted_class_idx)\n",
        "\n",
        "    # Get the class name using the predicted index\n",
        "    predicted_class_name = list(class_labels.keys())[list(class_labels.values()).index(predicted_class_idx)]\n",
        "\n",
        "    # Calculate the confidence percentage of the predicted class\n",
        "    confidence = predictions[0][predicted_class_idx] * 100  # Assuming softmax output, multiply by 100 for percentage\n",
        "    print(\"Confidence (%): {:.2f}%\".format(confidence))\n",
        "\n",
        "    return predicted_class_name, confidence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21pmxLEvQ3tO"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/cover/26.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=30, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "catcD3MLcPhm"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/defense/9.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=8, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM7jVTkbcPhm"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/flick/26.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=8, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn3lj3e3cPhm"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/hook/4.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=8, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlWqy2eUcPhs"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/late_cut/15.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=16, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYyEWLrBcPhs"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/lofted/7.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=16, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktyFJeZEcPht"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/square_cut/242.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=16, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obMMamFycPht"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/straight/295.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=16, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsrv3pBbcPhz"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/sweep/267.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=16, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu-TWIHPcPhz"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/dataset/test/pull/178.mp4'\n",
        "predicted_class, confidence = classify_video(video_path, model, frame_count=16, class_labels=classes)\n",
        "print(f\"The predicted class for the video is: {predicted_class}\", f\"Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5ccpuwNcPhz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "# Load pre-trained EfficientNetB0 without the top layer to use as a feature extractor\n",
        "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# Set the base model as non-trainable\n",
        "base_model.trainable = False\n",
        "\n",
        "# Define the full model using a Sequential model\n",
        "model = models.Sequential([\n",
        "    # Apply EfficientNetB0 to each frame of the video\n",
        "    layers.TimeDistributed(base_model, input_shape=(None, 224, 224, 3)),\n",
        "    layers.TimeDistributed(layers.GlobalAveragePooling2D()),\n",
        "\n",
        "    # Use GRU layers to capture temporal relationships\n",
        "    layers.GRU(256, return_sequences=True),\n",
        "    layers.GRU(128),\n",
        "\n",
        "    # Dense layers for classification\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1U94ldQcPiF"
      },
      "outputs": [],
      "source": [
        "# Path to the saved weights\n",
        "weights_path = '/content/models/Custom3_20_weights.h5'\n",
        "\n",
        "# Load the weights into the model\n",
        "model.load_weights(weights_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}